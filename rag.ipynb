{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f3b47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d74017ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0629a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL ANSWER ---\n",
      "\n",
      "Machine learning is not explicitly defined in the given context. However, the context does mention several applications of machine learning within the framework of LLM-powered autonomous agents, such as using LLMs as routers in MRKL systems, leveraging them for scientific discovery, and employing them for tasks like designing experiments and using tools.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load variables from .env\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Step 1: Load the web document\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\"],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "    )\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: Split document into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Step 3: Generate embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 4: Store in FAISS vector store\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Step 5: Similarity Search\n",
    "query = \"What is Machine learning\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "# Step 6: Prepare context for LLM\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "# Step 7: Set up Gemini as LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",  # or \"gemini-1.5-flash\" for a faster, cheaper option\n",
    "    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Step 8: Prepare prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant. Use the context below to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "prompt = prompt_template.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "# Step 9: Call the LLM\n",
    "response = llm.invoke(prompt)\n",
    "print(\"\\n--- FINAL ANSWER ---\\n\")\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
